#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Mar 10 16:06:49 2020

@author: garima
"""

import keras
from keras import layers
from keras.layers import *
from keras.datasets import cifar10
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import clear_output
import os

import glob, cv2, os

#define global parameters
SPATIAL_DIM =64 #Spatial dimension of the images
LATENT_DIM =100 # Dimensionality of the noise vector
BATCH_SIZE=32 #Batchsize to use for training
DISC_UPDATES=1 #number of discriminator updates per training iteration.
GEN_UPDATES= 1 # number of generator updates per training iteration.
FILTER_SIZE =5 # Filter Size to be applied throughout all convolution layers
NUM_LOAD= 1000 # Number of images to load from CelebA.
NET_CAPACITY =16 #General factor to globally change the number of convolutional filters
PROGRESS_INTERVAL = 80 # Number of iterations after which current samples will be plotted
ROOT_DIR = 'GAN'

os.getcwdb()

if not os.path.isdir(ROOT_DIR):
    os.mkdir(ROOT_DIR)

#Prepare data
def plot_image(x):
    plt.imshow(x*0.5+0.5)
    

X=[]
faces=glob.glob('/Users/garima/GAN/2019-computefest/Wednesday/auto_encoder/celeba/img_align_celeba/*.jpg')

for i,f in enumerate(faces):
    img=cv2.imread(f)
    img = cv2.resize(img, (SPATIAL_DIM,SPATIAL_DIM))
    img=np.flip(img,axis=2)
    img=img.astype(np.float32)/ 127.5-1.0
    X.append(img)
    if i>= NUM_LOAD -1:
        break
    
X=np.array(X)
plot_image(X[8])
X.shape, X.min(), X.max()

#Define Architecture

def add_encoder_block(x, filters, filter_size):
    x=Conv2D(filters, filter_size, padding='same')(x)
    x= BatchNormalization()(x)
    x=Conv2D(filters, filter_size, padding='same', strides=2)(x)
    x=BatchNormalization()(x)
    x=LeakyReLU(0.3)(x)
    return x
  
    # build the discriminator

def build_discriminator(start_filters, spatial_dim, filter_size):
    inp= Input(shape=(spatial_dim,spatial_dim,3))
    
    #Encoding blocks downsample the image.
    x=add_encoder_block(inp,start_filters, filter_size)
    # gradually increasing the filter size
    x=add_encoder_block(x,start_filters*2, filter_size)
    x=add_encoder_block(x,start_filters*4, filter_size)
    x=add_encoder_block(inp,start_filters*8, filter_size)
    
    x= GlobalAveragePooling2D()(x)
    x= Dense(1,activation='sigmoid') (x)
    return keras.Model(inputs=inp, outputs=x)

discriminator = build_discriminator (NET_CAPACITY, SPATIAL_DIM, FILTER_SIZE)
discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002), metrics=['mae'])
discriminator.summary()   
    # build thedecoder block
def add_decoder_block(x, filters, filter_size):
    x= Deconvolution2D(filters, filter_size, strides=2, padding='same')(x)
    x=BatchNormalization()(x)
    x=LeakyReLU(0.3)(x)
    return x
    
#build the generator - now use the decoder blocks and gradually decrease the filter size.

def build_generator(start_filters, filter_size, latent_dim):
    inp = Input(shape=(latent_dim,))
    
    # Projection.
    x = Dense(4 * 4 * (start_filters * 8), input_dim=latent_dim)(inp)
    x = BatchNormalization()(x)
    x = Reshape(target_shape=(4, 4, start_filters * 8))(x)
    
    # Decoding blocks upsample the image.
    x = add_decoder_block(x, start_filters * 4, filter_size)
    x = add_decoder_block(x, start_filters * 2, filter_size)
    x = add_decoder_block(x, start_filters, filter_size)
    x = add_decoder_block(x, start_filters, filter_size)    
    
    x = Conv2D(3, kernel_size=5, padding='same', activation='tanh')(x)
    return keras.Model(inputs=inp, outputs=x)

generator= build_generator(NET_CAPACITY,FILTER_SIZE, LATENT_DIM )  
generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002))
generator.summary()


#build GAN

gan= keras.Sequential()
gan.add(generator)
gan.add(discriminator)
    # Fix the discriminator part in the full setup.
discriminator.trainable= False 
gan.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(lr=0.0002),metrics=['mae']) 
gan.summary()
# Training
    
def construct_models(verbose=False):
   
    #1. Build Discriminator
    discriminator = build_discriminator (NET_CAPACITY, SPATIAL_DIM, FILTER_SIZE)
    discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002), metrics=['mae'])
      
    #2. Build generator
    generator= build_generator(NET_CAPACITY,FILTER_SIZE, LATENT_DIM )  
    #generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002), metrics=['mae'])
   #3. Build full GAN setup by stacking generator and discriminator.
    gan= keras.Sequential()
    gan.add(generator)
    gan.add(discriminator)
    # Fix the discriminator part in the full setup.
    discriminator.trainable= False 
    gan.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(lr=0.0002),metrics=['mae']) 
    
    #Print model summaries for debugging purposes.
    if verbose:     
        generator.summary()
        discriminator.summary()
        gan.summary()
    return generator, discriminator, gan

def run_training(start_it=0,num_epochs=1000):
    config_name= 'gan_cap' + str(NET_CAPACITY) + '_batch' + str(BATCH_SIZE) + '_filt' + str(FILTER_SIZE) + '_disc' +str(DISC_UPDATES) + '_gen' + str(GEN_UPDATES)
    
    folder = os.path.join(ROOT_DIR, config_name)
    
    if not os.path.isdir(folder):
        os.mkdir(folder)
     
    #initiate loop variables
    avg_loss_discriminator =[]
    avg_loss_generator =[]
    total_it = start_it 
    
    #Start training loop
    for epoch in range(num_epochs):
        loss_discriminator= []
        loss_generator =[]
        for it in range(200):
            
            #update discriminator
            for i in range(DISC_UPDATES):
                #fetch real examples 
                imgs_real= X[np.random.randint(0,X.shape[0], size=BATCH_SIZE)]
                
                #Generate fake examples.
                noise = np.random.randn(BATCH_SIZE, LATENT_DIM)
               
                imgs_fake = generator.predict(noise)
                
                d_loss_real= discriminator.train_on_batch(imgs_real, np.ones([BATCH_SIZE]))[1]
                d_loss_fake= discriminator.train_on_batch(imgs_fake, np.zeros([BATCH_SIZE]))[1]
    
            # Progress visualizations
            if total_it % PROGRESS_INTERVAL ==0:
                plt.figure(figsize=(5,2))
                #we sample seperate images
                num_vis=min(BATCH_SIZE,8)
                imgs_real= X[np.random.randint(0, X.shape[0], size=num_vis)]
                noise =np.random.randn(num_vis, LATENT_DIM)
                imgs_fake = generator.predict(noise)
                for obj_plot in [imgs_fake, imgs_real]:
                    plt.figure(figsize=(num_vis*3,3))
                    for b in range(num_vis):
                        disc_score = float(discriminator.predict(np.expand_dims(obj_plot[b], axis=0))[0])
                        plt.subplot(1, num_vis, b+1)
                        plt.title(str(round(disc_score,3)))
                        plot_image(obj_plot[b])
                    if obj_plot is imgs_fake:
                        plt.savefig(os.path.join(folder, str(total_it).zfill(10)+ '.jpg'), bbox_inches='tight')
                    plt.show()
                    
            #Update generator
            loss=0
            y = np.ones([BATCH_SIZE,1])
            for j in range(GEN_UPDATES):
                noise = np.random.randn(BATCH_SIZE, LATENT_DIM)
                loss += gan.train_on_batch(noise, y)[1]
                
            loss_discriminator.append((d_loss_real + d_loss_fake)/2.)
            loss_generator.append(loss/GEN_UPDATES)
            total_it +=1
                
                
            # Progress visualizetion
        clear_output(True)
        print('Epoch', epoch)
        avg_loss_discriminator.append(np.mean(loss_discriminator))
        avg_loss_generator.append(np.mean(loss_generator))
        plt.plot(range(len(avg_loss_discriminator)), avg_loss_discriminator)
        plt.plot(range(len(avg_loss_generator)), avg_loss_generator)
        plt.legend(['discriminator loss', 'generator loss'])
        plt.show()
    
generator, discriminator, gan = construct_models(verbose=True)
gan.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(lr=0.0002),metrics=['mae']) 
run_training()


    
    
    
    
    
                          
                          
                          